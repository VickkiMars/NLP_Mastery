{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOFGaMU8YUuObpppT9Fkr15",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VickkiMars/NLP_Mastery/blob/main/nmt_with_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8KGad-k4OL8x"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import pathlib\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow.data as tf_data\n",
        "import tensorflow.strings as tf_strings\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import ops\n",
        "from keras.layers import TextVectorization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"dataset_2.txt\", \"r\") as f:\n",
        "  lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "  inp, targ = line.split(\"\\t\")\n",
        "  targ = \"[start] \" + targ + \" [end]\"\n",
        "  text_pairs.append((inp, targ))"
      ],
      "metadata": {
        "id": "gO9xiApaOyoD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(5):\n",
        "  print(random.choice(text_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QccLQS1PAGR",
        "outputId": "9f8df0b2-0251-4d0c-996d-802874861f7d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Please make a transfer of 8h to Peace Microfinance Bank at Loucas Domhnall, account number 9793807269.', '[start] 800 Peace Microfinance Bank 9793807269 Loucas Domhnall [end]')\n",
            "('Please send 131k to 1146429305, Abbey Mortgage Bank, at Chiaka Uju.', '[start] 131000 Abbey Mortgage Bank 1146429305 Chiaka Uju [end]')\n",
            "('please transfer 76k to 2036822496', '[start] 76000 2036822496 [end]')\n",
            "('Deposit 54k to Amucha MFB, account number 4104402882, for Angels Mukolu', '[start] 54000 Amucha MFB 4104402882 Angels Mukolu [end]')\n",
            "('abeg send 34k to 7234499301', '[start] 34000 7234499301 [end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.2 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MErMMTvPZDZ",
        "outputId": "bf1e2205-bc85-4f9d-aa99-5510b3e60b41"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38985 total pairs\n",
            "23391 training pairs\n",
            "7797 validation pairs\n",
            "7797 test pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strip_chars = string.punctuation + \"Â¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")"
      ],
      "metadata": {
        "id": "s5kJPRtnQATV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 15000\n",
        "sequence_length = 140\n",
        "batch_size = 16\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "  lowercase = tf_strings.lower(input_string)\n",
        "  return tf_strings.regex_replace(\n",
        "      lowercase, f\"[{re.escape(strip_chars)}]\", \"\")"
      ],
      "metadata": {
        "id": "_qfYyowiQfGN"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "targ_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")"
      ],
      "metadata": {
        "id": "NDKY4TQtQr2c"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_inp_texts = [pair[0] for pair in train_pairs]\n",
        "train_targ_texts = [pair[1] for pair in train_pairs]\n",
        "inp_vectorization.adapt(train_inp_texts)\n",
        "targ_vectorization.adapt(train_targ_texts)"
      ],
      "metadata": {
        "id": "ruoi91WKQ3Uy"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_dataset(inp, targ):\n",
        "  inp = inp_vectorization(inp)\n",
        "  targ = targ_vectorization(targ)\n",
        "  return (\n",
        "    {\n",
        "      \"encoder_inputs\": inp,\n",
        "      \"decoder_inputs\": targ[:, :-1],\n",
        "    },\n",
        "    targ[:, 1:],\n",
        "  )"
      ],
      "metadata": {
        "id": "aGIbxnvhQ9DH"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dataset(pairs):\n",
        "  inp_texts, targ_texts = zip(*pairs)\n",
        "  inp_texts = list(inp_texts)\n",
        "  targ_texts = list(targ_texts)\n",
        "  dataset = tf_data.Dataset.from_tensor_slices((inp_texts, targ_texts))\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "  return dataset.shuffle(2048).prefetch(16)"
      ],
      "metadata": {
        "id": "W7uKbjVYRI5q"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "metadata": {
        "id": "c7ltG3m5RZ8a"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "  print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "  print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "  print(f\"targets.shape: {targets.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJgEKW90Rg2x",
        "outputId": "c2caab2c-374c-4f13-f5f9-1d82d8401262"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (16, 140)\n",
            "inputs[\"decoder_inputs\"].shape: (16, 140)\n",
            "targets.shape: (16, 140)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim\n",
        "    self.dense_dim = dense_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.attention = layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_dim\n",
        "    )\n",
        "    self.dense_proj = keras.Sequential(\n",
        "        [\n",
        "          layers.Dense(dense_dim, activation=\"relu\"),\n",
        "          layers.Dense(embed_dim),\n",
        "        ]\n",
        "    )\n",
        "    self.layernorm_1 = layers.LayerNormalization()\n",
        "    self.layernorm_2 = layers.LayerNormalization()\n",
        "    self.supports_masking = True\n",
        "\n",
        "  def call(self, inputs, mask=None):\n",
        "    if mask is not None:\n",
        "      padding_mask = ops.cast(mask[:, None, :], dtype='int32')\n",
        "    else:\n",
        "      padding_mask = None\n",
        "\n",
        "    attention_output = self.attention(\n",
        "        query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
        "    )\n",
        "    proj_input = self.layernorm_1(inputs + attention_output)\n",
        "    proj_output = self.dense_proj(proj_input)\n",
        "    return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update(\n",
        "        {\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        }\n",
        "    )\n",
        "    return config"
      ],
      "metadata": {
        "id": "kE4eL-3oRy8I"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "  def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.token_embeddings = layers.Embedding(\n",
        "        input_dim=vocab_size, output_dim=embed_dim\n",
        "    )\n",
        "    self.position_embeddings = layers.Embedding(\n",
        "        input_dim = sequence_length, output_dim = embed_dim\n",
        "    )\n",
        "    self.sequence_length = sequence_length\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embed_dim = embed_dim\n",
        "\n",
        "  def call(self, inputs):\n",
        "    length = ops.shape(inputs)[-1]\n",
        "    positions = ops.arange(0, length, 1)\n",
        "    embedded_tokens = self.token_embeddings(inputs)\n",
        "    embedded_positions = self.position_embeddings(positions)\n",
        "    return embedded_tokens + embedded_positions\n",
        "\n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "    if mask is None:\n",
        "      return None\n",
        "    else:\n",
        "      return ops.not_equal(inputs, 0)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update(\n",
        "        {\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"vocab_size\": self.vocab_size,\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "        }\n",
        "    )\n",
        "    return config"
      ],
      "metadata": {
        "id": "-rr8z-pTS_0X"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "  def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim\n",
        "    self.latent_dim = latent_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.attention_1 = layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_dim\n",
        "    )\n",
        "    self.attention_2 = layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_dim\n",
        "    )\n",
        "    self.dense_proj = keras.Sequential(\n",
        "        [\n",
        "          layers.Dense(latent_dim, activation=\"relu\"),\n",
        "          layers.Dense(embed_dim),\n",
        "        ]\n",
        "    )\n",
        "    self.layernorm_1 = layers.LayerNormalization()\n",
        "    self.layernorm_2 = layers.LayerNormalization()\n",
        "    self.layernorm_3 = layers.LayerNormalization()\n",
        "    self.supports_masking = True\n",
        "\n",
        "  def call(self, inputs, encoder_outputs, mask=None):\n",
        "    causal_mask = self.get_causal_attention_mask(inputs)\n",
        "    if mask is not None:\n",
        "      padding_mask = ops.cast(mask[:, None, :], dtype=\"int32\")\n",
        "      padding_mask = ops.minimum(\n",
        "          padding_mask, causal_mask\n",
        "      )\n",
        "    else:\n",
        "      padding_mask = None\n",
        "\n",
        "    attention_output_1 = self.attention_1(\n",
        "        query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
        "    )\n",
        "    out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "    attention_output_2 = self.attention_2(\n",
        "        query=out_1,\n",
        "        value=encoder_outputs,\n",
        "        key=encoder_outputs,\n",
        "        attention_mask=padding_mask,\n",
        "    )\n",
        "    out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "    proj_output = self.dense_proj(out_2)\n",
        "    return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "  def get_causal_attention_mask(self, inputs):\n",
        "    input_shape = ops.shape(inputs)\n",
        "    batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "    i = ops.arange(sequence_length)[:, None]\n",
        "    j = ops.arange(sequence_length)\n",
        "    mask = ops.cast(i >= j, dtype=\"int32\")\n",
        "    mask = ops.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "    mult = ops.concatenate(\n",
        "        [ops.expand_dims(batch_size, -1), ops.convert_to_tensor([1, 1])],\n",
        "        axis=0,\n",
        "    )\n",
        "    return ops.tile(mask, mult)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update(\n",
        "        {\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"latent_dim\": self.latent_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "        }\n",
        "    )\n",
        "    return config\n",
        "\n"
      ],
      "metadata": {
        "id": "eyif5RkOT5CS"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 128\n",
        "latent_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "transformer = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        ")"
      ],
      "metadata": {
        "id": "NU1SmwpOV3n0"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5  # This should be at least 30 for convergence\n",
        "\n",
        "transformer.summary()\n",
        "transformer.compile(\n",
        "    \"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "id": "z7bopNjGWTDB",
        "outputId": "01a7985e-abd0-4cdd-d525-f7ec70b7f360"
      },
      "execution_count": 52,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"transformer\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âââââââââââââââââââââââââââââ³âââââââââââââââââââââââââ³âââââââââââââââââ³âââââââââââââââââââââââââ\n",
              "â<span style=\"font-weight: bold\"> Layer (type)              </span>â<span style=\"font-weight: bold\"> Output Shape           </span>â<span style=\"font-weight: bold\">        Param # </span>â<span style=\"font-weight: bold\"> Connected to           </span>â\n",
              "â¡âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©\n",
              "â encoder_inputs            â (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           â              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â -                      â\n",
              "â (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              â                        â                â                        â\n",
              "âââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââ¼âââââââââââââââââ¼âââââââââââââââââââââââââ¤\n",
              "â positional_embedding_8    â (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      â      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,937,920</span> â encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â\n",
              "â (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbedding</span>)     â                        â                â                        â\n",
              "âââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââ¼âââââââââââââââââ¼âââââââââââââââââââââââââ¤\n",
              "â decoder_inputs            â (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           â              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â -                      â\n",
              "â (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              â                        â                â                        â\n",
              "âââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââ¼âââââââââââââââââ¼âââââââââââââââââââââââââ¤\n",
              "â transformer_encoder_4     â (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      â      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,054,464</span> â positional_embedding_â¦ â\n",
              "â (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)      â                        â                â                        â\n",
              "âââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââ¼âââââââââââââââââ¼âââââââââââââââââââââââââ¤\n",
              "â functional_17             â (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15000</span>)    â      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,455,128</span> â decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  â\n",
              "â (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)              â                        â                â transformer_encoder_4â¦ â\n",
              "âââââââââââââââââââââââââââââ´âââââââââââââââââââââââââ´âââââââââââââââââ´âââââââââââââââââââââââââ\n",
              "</pre>\n"
            ],
            "text/plain": [
              "âââââââââââââââââââââââââââââ³âââââââââââââââââââââââââ³âââââââââââââââââ³âââââââââââââââââââââââââ\n",
              "â\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0mâ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0mâ\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0mâ\n",
              "â¡âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©\n",
              "â encoder_inputs            â (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           â              \u001b[38;5;34m0\u001b[0m â -                      â\n",
              "â (\u001b[38;5;33mInputLayer\u001b[0m)              â                        â                â                        â\n",
              "âââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââ¼âââââââââââââââââ¼âââââââââââââââââââââââââ¤\n",
              "â positional_embedding_8    â (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      â      \u001b[38;5;34m1,937,920\u001b[0m â encoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â\n",
              "â (\u001b[38;5;33mPositionalEmbedding\u001b[0m)     â                        â                â                        â\n",
              "âââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââ¼âââââââââââââââââ¼âââââââââââââââââââââââââ¤\n",
              "â decoder_inputs            â (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           â              \u001b[38;5;34m0\u001b[0m â -                      â\n",
              "â (\u001b[38;5;33mInputLayer\u001b[0m)              â                        â                â                        â\n",
              "âââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââ¼âââââââââââââââââ¼âââââââââââââââââââââââââ¤\n",
              "â transformer_encoder_4     â (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      â      \u001b[38;5;34m1,054,464\u001b[0m â positional_embedding_â¦ â\n",
              "â (\u001b[38;5;33mTransformerEncoder\u001b[0m)      â                        â                â                        â\n",
              "âââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââ¼âââââââââââââââââ¼âââââââââââââââââââââââââ¤\n",
              "â functional_17             â (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15000\u001b[0m)    â      \u001b[38;5;34m5,455,128\u001b[0m â decoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  â\n",
              "â (\u001b[38;5;33mFunctional\u001b[0m)              â                        â                â transformer_encoder_4â¦ â\n",
              "âââââââââââââââââââââââââââââ´âââââââââââââââââââââââââ´âââââââââââââââââ´âââââââââââââââââââââââââ\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,447,512</span> (32.22 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,447,512\u001b[0m (32.22 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,447,512</span> (32.22 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,447,512\u001b[0m (32.22 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1462/1462\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 48ms/step - accuracy: 0.9608 - loss: 0.8210 - val_accuracy: 0.9747 - val_loss: 0.1440\n",
            "Epoch 2/5\n",
            "\u001b[1m1462/1462\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 43ms/step - accuracy: 0.9714 - loss: 0.1956 - val_accuracy: 0.9757 - val_loss: 0.1370\n",
            "Epoch 3/5\n",
            "\u001b[1m1462/1462\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 40ms/step - accuracy: 0.9722 - loss: 0.1849 - val_accuracy: 0.9762 - val_loss: 0.1352\n",
            "Epoch 4/5\n",
            "\u001b[1m1462/1462\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.9729 - loss: 0.1757 - val_accuracy: 0.9761 - val_loss: 0.1352\n",
            "Epoch 5/5\n",
            "\u001b[1m1462/1462\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 41ms/step - accuracy: 0.9735 - loss: 0.1668 - val_accuracy: 0.9762 - val_loss: 0.1364\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7eba4ee54a60>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targ_vocab = targ_vectorization.get_vocabulary()\n",
        "targ_index_lookup = dict(zip(range(len(targ_vocab)), targ_vocab))\n",
        "max_decoded_sentence_length = 140"
      ],
      "metadata": {
        "id": "neYNbFqyWiAr"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = inp_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = targ_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "\n",
        "        # ops.argmax(predictions[0, i, :]) is not a concrete value for jax here\n",
        "        sampled_token_index = ops.convert_to_numpy(\n",
        "            ops.argmax(predictions[0, i, :])\n",
        "        ).item(0)\n",
        "        sampled_token = targ_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "TvoMdEW6adv0"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(10):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    translated = decode_sequence(input_sentence)\n",
        "    print(input_sentence)\n",
        "    print(translated)\n",
        "    print(\"-\"*10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqZkP22GawDK",
        "outputId": "a57f3922-914e-4805-8f77-9e07bfdfade0"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please send 220k to 5570699237, at Waya Microfinance Bank\n",
            "[start] 5000 mayfair finance bank limited [UNK] [UNK] [end]\n",
            "----------\n",
            "Kindly send 221m to ISUA MFB, 1105148297, account Okpoko Cris\n",
            "[start] 5000 mayfair finance bank limited [UNK] [UNK] [end]\n",
            "----------\n",
            "Please send exactly 4h to Ugbong Alez at 8007028386, Fedeth MFB\n",
            "[start] 5000 mayfair finance bank limited [UNK] [UNK] [end]\n",
            "----------\n",
            "Deposit 8k to 8473423119, account number First City Monument Bank, for Bonilla Usaman\n",
            "[start] 5000 mayfair finance bank limited [UNK] [UNK] [end]\n",
            "----------\n",
            "I need you to transfer 57k to Lao Usman Ali, Sterling Bank, 2769729384 \n",
            "[start] 5000 mayfair finance bank limited [UNK] [UNK] [end]\n",
            "----------\n",
            "LOMA MFB, account number 5177889316, Name: Afunwa Rhiannan, please send 798k.\n",
            "[start] 5000 mayfair finance bank limited [UNK] [UNK] [end]\n",
            "----------\n",
            "please wire 326k at 8465917357 asap\n",
            "[start] 5000 mayfair finance bank limited [UNK] [UNK] [end]\n",
            "----------\n",
            "Please make a transfer of 220k to Fairmoney Microfinance Bank at Nayda Va, account number 1052955357.\n",
            "[start] 5000 mayfair finance bank limited [UNK] [UNK] [end]\n",
            "----------\n",
            "I need you to transfer 92m to Anibaba Yamil, RANDALPHA MICROFINANCE BANK, 2406205931 \n",
            "[start] 5000 mayfair finance bank limited [UNK] [UNK] [end]\n",
            "----------\n",
            "Beststar Microfinance Bank, account number 1773533406, Name: Melissa Toritseju, please send 853k.\n",
            "[start] 5000 mayfair finance bank limited [UNK] [UNK] [end]\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5JEwblUrmGjo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}